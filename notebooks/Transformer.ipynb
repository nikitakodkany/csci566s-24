{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### requirements"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kUN3F-XE1dqr",
        "outputId": "38d5f67f-aa91-48bc-bbc6-a0b7e1cc0450"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: sentence-transformers in c:\\users\\nikit\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (2.6.1)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.32.0 in c:\\users\\nikit\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sentence-transformers) (4.39.1)\n",
            "Requirement already satisfied: tqdm in c:\\users\\nikit\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sentence-transformers) (4.66.2)\n",
            "Requirement already satisfied: torch>=1.11.0 in c:\\users\\nikit\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sentence-transformers) (2.2.1)\n",
            "Requirement already satisfied: numpy in c:\\users\\nikit\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sentence-transformers) (1.26.4)\n",
            "Requirement already satisfied: scikit-learn in c:\\users\\nikit\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sentence-transformers) (1.4.1.post1)\n",
            "Requirement already satisfied: scipy in c:\\users\\nikit\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sentence-transformers) (1.12.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.15.1 in c:\\users\\nikit\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sentence-transformers) (0.22.1)\n",
            "Requirement already satisfied: Pillow in c:\\users\\nikit\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sentence-transformers) (10.2.0)\n",
            "Requirement already satisfied: filelock in c:\\users\\nikit\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (3.13.3)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\nikit\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (2024.3.1)\n",
            "Requirement already satisfied: packaging>=20.9 in c:\\users\\nikit\\appdata\\roaming\\python\\python310\\site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\nikit\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (6.0.1)\n",
            "Requirement already satisfied: requests in c:\\users\\nikit\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (2.31.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\nikit\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (4.10.0)\n",
            "Requirement already satisfied: sympy in c:\\users\\nikit\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (1.12)\n",
            "Requirement already satisfied: networkx in c:\\users\\nikit\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in c:\\users\\nikit\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.1.3)\n",
            "Requirement already satisfied: colorama in c:\\users\\nikit\\appdata\\roaming\\python\\python310\\site-packages (from tqdm->sentence-transformers) (0.4.6)\n",
            "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\nikit\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers<5.0.0,>=4.32.0->sentence-transformers) (2023.12.25)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in c:\\users\\nikit\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers<5.0.0,>=4.32.0->sentence-transformers) (0.15.2)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\nikit\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers<5.0.0,>=4.32.0->sentence-transformers) (0.4.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\nikit\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-learn->sentence-transformers) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\nikit\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-learn->sentence-transformers) (3.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\nikit\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\nikit\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\nikit\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\nikit\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (2.2.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\nikit\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (2024.2.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in c:\\users\\nikit\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!python -m pip install -U sentence-transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "aKCF9Q56ue-G"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\nikit\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import math\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.optim import Adam\n",
        "from transformers import AutoModel, AutoTokenizer, AutoModelForSequenceClassification\n",
        "from sentence_transformers import SentenceTransformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "mmpWO07qaY1q"
      },
      "outputs": [],
      "source": [
        "class PreDUNES(nn.Module):\n",
        "    def __init__(\n",
        "            self,\n",
        "            twitter_embedding_model: nn.Module,\n",
        "            twitter_sentiment_tokenizer,\n",
        "            twitter_sentiment_model: nn.Module,\n",
        "            reddit_sentiment_tokenizer,\n",
        "            reddit_sentiment_model: nn.Module,\n",
        "            twitter_sector_tokenizer,\n",
        "            twitter_sector_model: nn.Module\n",
        "        ):\n",
        "        '''\n",
        "        Initialize a DUNES model from a set of embeddings and sentiment models.\n",
        "        Args:\n",
        "            twitter_embedding_model: huggingface model for Twitter embeddings\n",
        "            twitter_sentiment_model: huggingface model for Twitter sentiment\n",
        "            reddit_sentiment_model: huggingface model for Reddit sentiment\n",
        "            twitter_sector_model: huggingface model for Twitter sector\n",
        "        '''\n",
        "        super(PreDUNES, self).__init__()\n",
        "        self.twitter_embedding_model = twitter_embedding_model\n",
        "        self.twitter_sentiment_tokenizer = twitter_sentiment_tokenizer\n",
        "        self.twitter_sentiment_model = twitter_sentiment_model\n",
        "        self.reddit_sentiment_tokenizer = reddit_sentiment_tokenizer\n",
        "        self.reddit_sentiment_model = reddit_sentiment_model\n",
        "        self.twitter_sector_tokenizer = twitter_sector_tokenizer\n",
        "        self.twitter_sector_model = twitter_sector_model\n",
        "\n",
        "    def forward(self, prev_tweet, curr_tweet, prev_reddit):\n",
        "        '''\n",
        "        Forward pass for the DUNES model.\n",
        "        Args:\n",
        "            prev_tweet: previous tweet\n",
        "            curr_tweet: current tweet\n",
        "            prev_reddit: previous Reddit post\n",
        "            curr_reddit: current Reddit post\n",
        "        Returns:\n",
        "            sentiment: sentiment of the current tweet\n",
        "            sector: sector of the current tweet\n",
        "        '''\n",
        "        # Get the embeddings\n",
        "        prev_tweet_embedding = self.twitter_embedding_model.encode([prev_tweet])\n",
        "        curr_tweet_embedding = self.twitter_embedding_model.encode([curr_tweet])\n",
        "\n",
        "        # Get the sentiment\n",
        "        prev_tweet_tokens = self.twitter_sentiment_tokenizer(prev_tweet, return_tensors='pt')\n",
        "        prev_tweet_sentiment = self.twitter_sentiment_model(**prev_tweet_tokens)[0][0].detach().numpy()\n",
        "        curr_tweet_tokens = self.twitter_sentiment_tokenizer(curr_tweet, return_tensors='pt')\n",
        "        curr_tweet_sentiment = self.twitter_sentiment_model(**curr_tweet_tokens)[0][0].detach().numpy()\n",
        "\n",
        "        prev_reddit_tokens = self.reddit_sentiment_tokenizer(prev_reddit, return_tensors='pt')\n",
        "        prev_reddit_sentiment = self.reddit_sentiment_model(**prev_reddit_tokens)[0][0].detach().numpy()\n",
        "\n",
        "        # Get the sector\n",
        "        prev_sector_tokens = self.twitter_sector_tokenizer(prev_tweet, return_tensors='pt')\n",
        "        prev_tweet_sector = self.twitter_sector_model(**prev_sector_tokens)[0][0].detach().numpy()\n",
        "        curr_sector_tokens = self.twitter_sector_tokenizer(curr_tweet, return_tensors='pt')\n",
        "        curr_tweet_sector = self.twitter_sector_model(**curr_sector_tokens)[0][0].detach().numpy()\n",
        "\n",
        "\n",
        "        return prev_tweet_embedding, curr_tweet_embedding, prev_tweet_sentiment, curr_tweet_sentiment, prev_reddit_sentiment, prev_tweet_sector, curr_tweet_sector\n",
        "\n",
        "\n",
        "def create_preprocessing_model():\n",
        "    twitter_embedding = \"sentence-transformers/mixedbread-ai/mxbai-embed-large-v1\"\n",
        "    twitter_sentiment = \"cardiffnlp/twitter-roberta-base-sentiment-latest\"\n",
        "    reddit_sentiment = \"SamLowe/roberta-base-go_emotions\"\n",
        "    twitter_sector = \"cardiffnlp/tweet-topic-latest-multi\"\n",
        "\n",
        "    '''\n",
        "    Initialize a DUNES model from a set of embeddings and sentiment models.\n",
        "    Feeds the output of these models to a transformer for classification.\n",
        "    Args:\n",
        "        twitter_embedding: path to the Twitter embedding huggingface model\n",
        "        twitter_sentiment: path to the Twitter sentiment huggingface model\n",
        "        reddit_sentiment: path to the Reddit sentiment huggingface model\n",
        "        twitter_sector: path to the Twitter sector huggingface model\n",
        "    '''\n",
        "\n",
        "    # Load the models\n",
        "    twitter_embedding_model = SentenceTransformer(twitter_embedding)\n",
        "    twitter_sentiment_tokenizer = AutoTokenizer.from_pretrained(twitter_sentiment)\n",
        "    twitter_sentiment_model = AutoModelForSequenceClassification.from_pretrained(twitter_sentiment)\n",
        "    reddit_sentiment_tokenizer = AutoTokenizer.from_pretrained(reddit_sentiment)\n",
        "    reddit_sentiment_model = AutoModelForSequenceClassification.from_pretrained(reddit_sentiment)\n",
        "    twitter_sector_tokenizer = AutoTokenizer.from_pretrained(twitter_sector)\n",
        "    twitter_sector_model = AutoModelForSequenceClassification.from_pretrained(twitter_sector)\n",
        "\n",
        "    # Freeze the models\n",
        "    twitter_sentiment_model.eval()\n",
        "    twitter_sentiment_model.requires_grad_(False)\n",
        "    reddit_sentiment_model.eval()\n",
        "    reddit_sentiment_model.requires_grad_(False)\n",
        "    twitter_sector_model.eval()\n",
        "    twitter_sector_model.requires_grad_(False)\n",
        "\n",
        "    # Create the DUNES model\n",
        "    model = PreDUNES(\n",
        "        twitter_embedding_model,\n",
        "        twitter_sentiment_tokenizer,\n",
        "        twitter_sentiment_model,\n",
        "        reddit_sentiment_tokenizer,\n",
        "        reddit_sentiment_model,\n",
        "        twitter_sector_tokenizer,\n",
        "        twitter_sector_model\n",
        "        )\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "Path sentence-transformers/mixedbread-ai/mxbai-embed-large-v1 not found",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[11], line 13\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Example data list\u001b[39;00m\n\u001b[0;32m      2\u001b[0m data \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m      3\u001b[0m     {\n\u001b[0;32m      4\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprev_tweet\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m@WholeMarsBlog Headline is misleading. Starlink can obviously offer far more robust positioning than GPS, as it will have ~1000X more satellites over time. Not all will have line of sight to users, but still >10X GPS & far stronger signal. Just not today’s problem.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     10\u001b[0m     }\n\u001b[0;32m     11\u001b[0m ]\n\u001b[1;32m---> 13\u001b[0m preprocessing_model \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_preprocessing_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[1;32mIn[3], line 81\u001b[0m, in \u001b[0;36mcreate_preprocessing_model\u001b[1;34m()\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;124;03mInitialize a DUNES model from a set of embeddings and sentiment models.\u001b[39;00m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;124;03mFeeds the output of these models to a transformer for classification.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     77\u001b[0m \u001b[38;5;124;03m    twitter_sector: path to the Twitter sector huggingface model\u001b[39;00m\n\u001b[0;32m     78\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[0;32m     80\u001b[0m \u001b[38;5;66;03m# Load the models\u001b[39;00m\n\u001b[1;32m---> 81\u001b[0m twitter_embedding_model \u001b[38;5;241m=\u001b[39m \u001b[43mSentenceTransformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtwitter_embedding\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     82\u001b[0m twitter_sentiment_tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(twitter_sentiment)\n\u001b[0;32m     83\u001b[0m twitter_sentiment_model \u001b[38;5;241m=\u001b[39m AutoModelForSequenceClassification\u001b[38;5;241m.\u001b[39mfrom_pretrained(twitter_sentiment)\n",
            "File \u001b[1;32mc:\\Users\\nikit\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sentence_transformers\\SentenceTransformer.py:184\u001b[0m, in \u001b[0;36mSentenceTransformer.__init__\u001b[1;34m(self, model_name_or_path, modules, device, prompts, default_prompt_name, cache_folder, trust_remote_code, revision, token, use_auth_token)\u001b[0m\n\u001b[0;32m    181\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(model_name_or_path):\n\u001b[0;32m    182\u001b[0m     \u001b[38;5;66;03m# Not a path, load from hub\u001b[39;00m\n\u001b[0;32m    183\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m model_name_or_path \u001b[38;5;129;01mor\u001b[39;00m model_name_or_path\u001b[38;5;241m.\u001b[39mcount(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m--> 184\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPath \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m not found\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(model_name_or_path))\n\u001b[0;32m    186\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m model_name_or_path \u001b[38;5;129;01mand\u001b[39;00m model_name_or_path\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m basic_transformer_models:\n\u001b[0;32m    187\u001b[0m         \u001b[38;5;66;03m# A model from sentence-transformers\u001b[39;00m\n\u001b[0;32m    188\u001b[0m         model_name_or_path \u001b[38;5;241m=\u001b[39m __MODEL_HUB_ORGANIZATION__ \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m model_name_or_path\n",
            "\u001b[1;31mValueError\u001b[0m: Path sentence-transformers/mixedbread-ai/mxbai-embed-large-v1 not found"
          ]
        }
      ],
      "source": [
        "# Example data list\n",
        "data = [\n",
        "    {\n",
        "        'prev_tweet': \"@WholeMarsBlog Headline is misleading. Starlink can obviously offer far more robust positioning than GPS, as it will have ~1000X more satellites over time. Not all will have line of sight to users, but still >10X GPS & far stronger signal. Just not today’s problem.\",\n",
        "        'curr_tweet': \"@spideycyp_155 @BillyM2k If Russia faced calamitous defeat in conventional warfare for something as strategically critical as Crimea, the probability of using nuclear weapons is high\",\n",
        "        'prev_reddit': \"We know who controls the media. The same corporations who have wreaked havoc on the globe for decades, if not centuries, the big banks who financed them, and the governments who turned a blind eye to the destruction. The same entities who have brought us to the precipice of destruction - quite possibly condemning us, and our progeny to an unlivable climate They have tried to stop you at every turn, and yet you persist for the good of humanity. We love you, Elon! Keep up the good work! As you have said, we must never let the light of human consciousness fade - never!\",\n",
        "        'likes': 100,  \n",
        "        'retweets': 50, \n",
        "        'comments': 25  \n",
        "    }\n",
        "]\n",
        "\n",
        "preprocessing_model = create_preprocessing_model()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "class dataloaderDUNES(Dataset):\n",
        "    def __init__(self, data, preprocessing_model):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            data (List[Dict]): Each dictionary contains raw text for 'prev_tweet', 'curr_tweet',\n",
        "                               'prev_reddit', and engagement metrics ('likes', 'retweets', 'comments').\n",
        "            preprocessing_model (PreDUNES): The model instance for preprocessing text data.\n",
        "        \"\"\"\n",
        "        self.data = data\n",
        "        self.preprocessing_model = preprocessing_model\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        item = self.data[idx]\n",
        "        # Process the raw texts through the PreDUNES model\n",
        "        prev_tweet_embedding, curr_tweet_embedding, prev_tweet_sentiment, curr_tweet_sentiment, prev_reddit_sentiment, prev_tweet_sector, curr_tweet_sector = self.preprocessing_model(\n",
        "            item['prev_tweet'], item['curr_tweet'], item['prev_reddit']\n",
        "        )\n",
        "\n",
        "        # Convert numpy arrays to tensors if they are not already\n",
        "        def to_tensor(array):\n",
        "            if isinstance(array, np.ndarray):\n",
        "                return torch.tensor(array).float()\n",
        "            return array\n",
        "        \n",
        "        prev_tweet_embedding = to_tensor(prev_tweet_embedding)\n",
        "        curr_tweet_embedding = to_tensor(curr_tweet_embedding)\n",
        "        prev_tweet_sentiment = to_tensor(prev_tweet_sentiment)\n",
        "        curr_tweet_sentiment = to_tensor(curr_tweet_sentiment)\n",
        "        prev_reddit_sentiment = to_tensor(prev_reddit_sentiment)\n",
        "        prev_tweet_sector = to_tensor(prev_tweet_sector)\n",
        "        curr_tweet_sector = to_tensor(curr_tweet_sector)\n",
        "        \n",
        "        # Engagement metrics\n",
        "        likes = torch.tensor(item['likes'], dtype=torch.float)\n",
        "        retweets = torch.tensor(item['retweets'], dtype=torch.float)\n",
        "        comments = torch.tensor(item['comments'], dtype=torch.float)\n",
        "        \n",
        "        return {\n",
        "            'prev_tweet_embedding': prev_tweet_embedding,\n",
        "            'curr_tweet_embedding': curr_tweet_embedding,\n",
        "            'prev_tweet_sentiment': prev_tweet_sentiment,\n",
        "            'curr_tweet_sentiment': curr_tweet_sentiment,\n",
        "            'prev_reddit_sentiment': prev_reddit_sentiment,\n",
        "            'prev_tweet_sector': prev_tweet_sector,\n",
        "            'curr_tweet_sector': curr_tweet_sector,\n",
        "            'likes': likes,\n",
        "            'retweets': retweets,\n",
        "            'comments': comments\n",
        "        }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "Path sentence-transformers/mixedbread-ai/mxbai-embed-large-v1 not found",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[10], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m preprocessing_model \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_preprocessing_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[1;32mIn[3], line 81\u001b[0m, in \u001b[0;36mcreate_preprocessing_model\u001b[1;34m()\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;124;03mInitialize a DUNES model from a set of embeddings and sentiment models.\u001b[39;00m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;124;03mFeeds the output of these models to a transformer for classification.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     77\u001b[0m \u001b[38;5;124;03m    twitter_sector: path to the Twitter sector huggingface model\u001b[39;00m\n\u001b[0;32m     78\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[0;32m     80\u001b[0m \u001b[38;5;66;03m# Load the models\u001b[39;00m\n\u001b[1;32m---> 81\u001b[0m twitter_embedding_model \u001b[38;5;241m=\u001b[39m \u001b[43mSentenceTransformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtwitter_embedding\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     82\u001b[0m twitter_sentiment_tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(twitter_sentiment)\n\u001b[0;32m     83\u001b[0m twitter_sentiment_model \u001b[38;5;241m=\u001b[39m AutoModelForSequenceClassification\u001b[38;5;241m.\u001b[39mfrom_pretrained(twitter_sentiment)\n",
            "File \u001b[1;32mc:\\Users\\nikit\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sentence_transformers\\SentenceTransformer.py:184\u001b[0m, in \u001b[0;36mSentenceTransformer.__init__\u001b[1;34m(self, model_name_or_path, modules, device, prompts, default_prompt_name, cache_folder, trust_remote_code, revision, token, use_auth_token)\u001b[0m\n\u001b[0;32m    181\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(model_name_or_path):\n\u001b[0;32m    182\u001b[0m     \u001b[38;5;66;03m# Not a path, load from hub\u001b[39;00m\n\u001b[0;32m    183\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m model_name_or_path \u001b[38;5;129;01mor\u001b[39;00m model_name_or_path\u001b[38;5;241m.\u001b[39mcount(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m--> 184\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPath \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m not found\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(model_name_or_path))\n\u001b[0;32m    186\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m model_name_or_path \u001b[38;5;129;01mand\u001b[39;00m model_name_or_path\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m basic_transformer_models:\n\u001b[0;32m    187\u001b[0m         \u001b[38;5;66;03m# A model from sentence-transformers\u001b[39;00m\n\u001b[0;32m    188\u001b[0m         model_name_or_path \u001b[38;5;241m=\u001b[39m __MODEL_HUB_ORGANIZATION__ \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m model_name_or_path\n",
            "\u001b[1;31mValueError\u001b[0m: Path sentence-transformers/mixedbread-ai/mxbai-embed-large-v1 not found"
          ]
        }
      ],
      "source": [
        "dataset = DataLoaderDUNES(data, preprocessing_model)\n",
        "dataloader = DataLoader(dataset, batch_size=32, shuffle=True, num_workers=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        pe = torch.zeros(max_len, 1, d_model)\n",
        "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.pe[:x.size(0)]\n",
        "        return self.dropout(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class TransformerModel(nn.Module):\n",
        "    def __init__(self, feature_sizes, d_model, nhead, num_encoder_layers, dim_feedforward, num_outputs):\n",
        "        super(TransformerModel, self).__init__()\n",
        "        self.d_model = d_model\n",
        "        self.positional_encoder = PositionalEncoding(d_model)\n",
        "        \n",
        "        self.projection_layers = nn.ModuleDict({\n",
        "            'prev_tweet_embedding': nn.Linear(feature_sizes['tweet_embedding'], d_model),\n",
        "            'curr_tweet_embedding': nn.Linear(feature_sizes['tweet_embedding'], d_model),\n",
        "            'prev_tweet_sentiment': nn.Linear(feature_sizes['tweet_sentiment'], d_model),\n",
        "            'curr_tweet_sentiment': nn.Linear(feature_sizes['tweet_sentiment'], d_model),\n",
        "            'prev_reddit_sentiment': nn.Linear(feature_sizes['reddit_sentiment'], d_model),\n",
        "            'prev_tweet_sector': nn.Linear(feature_sizes['tweet_sector'], d_model),\n",
        "            'curr_tweet_sector': nn.Linear(feature_sizes['tweet_sector'], d_model),\n",
        "        })\n",
        "        \n",
        "        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead, dim_feedforward=dim_feedforward)\n",
        "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_encoder_layers)\n",
        "        self.output_linear = nn.Linear(d_model, num_outputs)\n",
        "\n",
        "    def forward(self, features):\n",
        "        projected_features = []\n",
        "        for key, feature in features.items():\n",
        "            # Project and reshape each feature\n",
        "            feature = self.projection_layers[key](feature)\n",
        "            if feature.dim() == 2:\n",
        "                feature = feature.unsqueeze(1)  # Add sequence dimension if missing\n",
        "            projected_features.append(feature)\n",
        "        \n",
        "        # Concatenate all features along the sequence dimension\n",
        "        src = torch.cat(projected_features, dim=1)\n",
        "        \n",
        "        # Apply positional encoding\n",
        "        src = self.positional_encoder(src)\n",
        "        \n",
        "        # Transformer encoder\n",
        "        output = self.transformer_encoder(src)\n",
        "        \n",
        "        # Aggregate and predict\n",
        "        output = output.mean(dim=0)\n",
        "        output = self.output_linear(output)\n",
        "        \n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Instantiate the model\n",
        "model = TransformerModel(\n",
        "    feature_sizes={\n",
        "        'tweet_embedding': 1024,\n",
        "        'tweet_sentiment': 3,\n",
        "        'reddit_sentiment': 28,\n",
        "        'tweet_sector': 19,\n",
        "    },\n",
        "    d_model=512, \n",
        "    nhead=8, \n",
        "    num_encoder_layers=3, \n",
        "    dim_feedforward=2048, \n",
        "    num_outputs=3\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Print the model's parameters\n",
        "for name, param in model.named_parameters():\n",
        "    print(f\"{name}: {param.size()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define the loss function\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "# Define the optimizer\n",
        "optimizer = Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Your training loop follows here\n",
        "num_epochs = 25"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for epoch in range(num_epochs):\n",
        "    # Training phase\n",
        "    model.train()  \n",
        "    train_loss = 0.0\n",
        "    for batch in dataloader:\n",
        "        optimizer.zero_grad() \n",
        "        features = {\n",
        "            'prev_tweet_embedding': batch['prev_tweet_embedding'],\n",
        "            'curr_tweet_embedding': batch['curr_tweet_embedding'],\n",
        "            'prev_tweet_sentiment': batch['prev_tweet_sentiment'],\n",
        "            'curr_tweet_sentiment': batch['curr_tweet_sentiment'],\n",
        "            'prev_reddit_sentiment': batch['prev_reddit_sentiment'],\n",
        "            'prev_tweet_sector': batch['prev_tweet_sector'],\n",
        "            'curr_tweet_sector': batch['curr_tweet_sector'],\n",
        "        }\n",
        "        targets = torch.stack((batch['likes'], batch['retweets'], batch['comments']), dim=1)\n",
        "        outputs = model(features)\n",
        "        loss = criterion(outputs, targets)\n",
        "        train_loss += loss.item()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    avg_train_loss = train_loss / len(dataloader)\n",
        "    print(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {avg_train_loss:.4f}')\n",
        "\n",
        "    # Prediction phase\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    with torch.no_grad():  # Disable gradient calculation\n",
        "        total_predictions = []\n",
        "        total_targets = []\n",
        "        for batch in validation_dataloader:\n",
        "            features = {\n",
        "                'prev_tweet_embedding': batch['prev_tweet_embedding'],\n",
        "                'curr_tweet_embedding': batch['curr_tweet_embedding'],\n",
        "                'prev_tweet_sentiment': batch['prev_tweet_sentiment'],\n",
        "                'curr_tweet_sentiment': batch['curr_tweet_sentiment'],\n",
        "                'prev_reddit_sentiment': batch['prev_reddit_sentiment'],\n",
        "                'prev_tweet_sector': batch['prev_tweet_sector'],\n",
        "                'curr_tweet_sector': batch['curr_tweet_sector'],\n",
        "            }\n",
        "            targets = torch.stack((batch['likes'], batch['retweets'], batch['comments']), dim=1)\n",
        "            predictions = model(features)\n",
        "            total_predictions.append(predictions)\n",
        "            total_targets.append(targets)\n",
        "        \n",
        "        # Optionally, convert predictions and targets to a convenient format for analysis, e.g., numpy arrays\n",
        "        predictions_np = torch.cat(total_predictions, dim=0).cpu().numpy()\n",
        "        targets_np = torch.cat(total_targets, dim=0).cpu().numpy()\n",
        "        # Now, predictions_np and targets_np can be used for further analysis, e.g., calculating metrics"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

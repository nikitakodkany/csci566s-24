{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kUN3F-XE1dqr",
        "outputId": "38d5f67f-aa91-48bc-bbc6-a0b7e1cc0450"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: sentence-transformers in c:\\users\\nikit\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (2.6.1)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.32.0 in c:\\users\\nikit\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sentence-transformers) (4.39.1)\n",
            "Requirement already satisfied: tqdm in c:\\users\\nikit\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sentence-transformers) (4.66.2)\n",
            "Requirement already satisfied: torch>=1.11.0 in c:\\users\\nikit\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sentence-transformers) (2.2.1)\n",
            "Requirement already satisfied: numpy in c:\\users\\nikit\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sentence-transformers) (1.26.4)\n",
            "Requirement already satisfied: scikit-learn in c:\\users\\nikit\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sentence-transformers) (1.4.1.post1)\n",
            "Requirement already satisfied: scipy in c:\\users\\nikit\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sentence-transformers) (1.12.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.15.1 in c:\\users\\nikit\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sentence-transformers) (0.22.1)\n",
            "Requirement already satisfied: Pillow in c:\\users\\nikit\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sentence-transformers) (10.2.0)\n",
            "Requirement already satisfied: filelock in c:\\users\\nikit\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (3.13.3)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\nikit\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (2024.3.1)\n",
            "Requirement already satisfied: packaging>=20.9 in c:\\users\\nikit\\appdata\\roaming\\python\\python310\\site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\nikit\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (6.0.1)\n",
            "Requirement already satisfied: requests in c:\\users\\nikit\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (2.31.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\nikit\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (4.10.0)\n",
            "Requirement already satisfied: sympy in c:\\users\\nikit\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (1.12)\n",
            "Requirement already satisfied: networkx in c:\\users\\nikit\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in c:\\users\\nikit\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.1.3)\n",
            "Requirement already satisfied: colorama in c:\\users\\nikit\\appdata\\roaming\\python\\python310\\site-packages (from tqdm->sentence-transformers) (0.4.6)\n",
            "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\nikit\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers<5.0.0,>=4.32.0->sentence-transformers) (2023.12.25)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in c:\\users\\nikit\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers<5.0.0,>=4.32.0->sentence-transformers) (0.15.2)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\nikit\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers<5.0.0,>=4.32.0->sentence-transformers) (0.4.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\nikit\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-learn->sentence-transformers) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\nikit\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-learn->sentence-transformers) (3.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\nikit\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\nikit\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\nikit\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\nikit\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (2.2.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\nikit\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (2024.2.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in c:\\users\\nikit\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!python -m pip install -U sentence-transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "aKCF9Q56ue-G"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/Users/mihir/Desktop/Classes/CSCI-566/Project/csci566s-24\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/homebrew/anaconda3/envs/dunes/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "%cd ..\n",
        "import math\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.optim import Adam\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from model.builder import create_preprocessing_model"
      ]
    },
    {
      "cell_type": "code",
<<<<<<< HEAD
      "execution_count": 13,
      "metadata": {
        "id": "mmpWO07qaY1q"
      },
      "outputs": [],
      "source": [
        "class PreDUNES(nn.Module):\n",
        "    def __init__(self, twitter_embedding_model, twitter_sentiment_tokenizer, twitter_sentiment_model, reddit_sentiment_tokenizer, reddit_sentiment_model, twitter_sector_tokenizer, twitter_sector_model):\n",
        "        super(PreDUNES, self).__init__()\n",
        "        self.twitter_embedding_model = twitter_embedding_model\n",
        "        self.twitter_sentiment_tokenizer = twitter_sentiment_tokenizer\n",
        "        self.twitter_sentiment_model = twitter_sentiment_model\n",
        "        self.reddit_sentiment_tokenizer = reddit_sentiment_tokenizer\n",
        "        self.reddit_sentiment_model = reddit_sentiment_model\n",
        "        self.twitter_sector_tokenizer = twitter_sector_tokenizer\n",
        "        self.twitter_sector_model = twitter_sector_model\n",
        "\n",
        "    def forward(self, prev_tweet, prev_reddit):\n",
        "        # Corrected to use convert_to_tensor=True\n",
        "        prev_tweet_embedding = self.twitter_embedding_model.encode([prev_tweet], convert_to_tensor=True)\n",
        "        prev_tweet_tokens = self.twitter_sentiment_tokenizer(prev_tweet, return_tensors='pt')\n",
        "        prev_tweet_sentiment = self.twitter_sentiment_model(**prev_tweet_tokens).logits\n",
        "        prev_reddit_tokens = self.reddit_sentiment_tokenizer(prev_reddit, return_tensors='pt')\n",
        "        prev_reddit_sentiment = self.reddit_sentiment_model(**prev_reddit_tokens).logits\n",
        "        prev_sector_tokens = self.twitter_sector_tokenizer(prev_tweet, return_tensors='pt')\n",
        "        prev_tweet_sector = self.twitter_sector_model(**prev_sector_tokens).logits\n",
        "\n",
        "        return prev_tweet_embedding, prev_tweet_sentiment, prev_reddit_sentiment, prev_tweet_sector\n",
        "\n",
        "\n",
        "\n",
        "def create_preprocessing_model(twitter_embedding, twitter_sentiment, reddit_sentiment, twitter_sector):\n",
        "    twitter_embedding_model = SentenceTransformer(twitter_embedding)\n",
        "    twitter_sentiment_tokenizer = AutoTokenizer.from_pretrained(twitter_sentiment)\n",
        "    twitter_sentiment_model = AutoModelForSequenceClassification.from_pretrained(twitter_sentiment)\n",
        "    reddit_sentiment_tokenizer = AutoTokenizer.from_pretrained(reddit_sentiment)\n",
        "    reddit_sentiment_model = AutoModelForSequenceClassification.from_pretrained(reddit_sentiment)\n",
        "    twitter_sector_tokenizer = AutoTokenizer.from_pretrained(twitter_sector)\n",
        "    twitter_sector_model = AutoModelForSequenceClassification.from_pretrained(twitter_sector)\n",
        "\n",
        "    # Set models to evaluation mode\n",
        "    twitter_sentiment_model.eval()\n",
        "    reddit_sentiment_model.eval()\n",
        "    twitter_sector_model.eval()\n",
        "\n",
        "    # Disable gradients for models\n",
        "    for model in [twitter_sentiment_model, reddit_sentiment_model, twitter_sector_model]:\n",
        "        for param in model.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "    model = PreDUNES(\n",
        "        twitter_embedding_model,\n",
        "        twitter_sentiment_tokenizer,\n",
        "        twitter_sentiment_model,\n",
        "        reddit_sentiment_tokenizer,\n",
        "        reddit_sentiment_model,\n",
        "        twitter_sector_tokenizer,\n",
        "        twitter_sector_model\n",
        "    )\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 14,
=======
      "execution_count": 64,
>>>>>>> 008bb37956ddcdf15f56edc22f1476b4868122bc
      "metadata": {},
      "outputs": [],
      "source": [
        "class DataLoaderDUNES(Dataset):\n",
        "    def __init__(self, data, preprocessing_model):\n",
        "        self.data = data\n",
        "        self.preprocessing_model = preprocessing_model\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        item = self.data[idx]\n",
        "        prev_tweet_embedding, prev_tweet_sentiment, prev_reddit_sentiment, prev_tweet_sector = self.preprocessing_model(\n",
        "            item['prev_tweet'], item['prev_reddit']\n",
        "        )\n",
        "\n",
        "        prev_tweet_embedding, prev_tweet_sentiment, prev_reddit_sentiment, prev_tweet_sector = self.preprocessing_model(\n",
        "            item['prev_tweet'], item['prev_reddit']\n",
        "        )\n",
        "\n",
        "        def to_tensor(obj):\n",
        "            if not isinstance(obj, torch.Tensor):\n",
        "                obj = torch.tensor(obj, dtype=torch.float)\n",
        "            return obj\n",
        "\n",
        "        prev_tweet_embedding = to_tensor(prev_tweet_embedding)\n",
        "        prev_tweet_sentiment = to_tensor(prev_tweet_sentiment)\n",
        "        prev_reddit_sentiment = to_tensor(prev_reddit_sentiment)\n",
        "        prev_tweet_sector = to_tensor(prev_tweet_sector)\n",
        "\n",
        "        return {\n",
        "            'prev_tweet_embedding': prev_tweet_embedding.squeeze(),\n",
        "            'prev_tweet_sentiment': prev_tweet_sentiment,\n",
        "            'prev_reddit_sentiment': prev_reddit_sentiment,\n",
        "            'prev_tweet_sector': prev_tweet_sector,\n",
        "            'likes': torch.tensor(item['likes'], dtype=torch.float),\n",
        "            'retweets': torch.tensor(item['retweets'], dtype=torch.float),\n",
        "            'comments': torch.tensor(item['comments'], dtype=torch.float)\n",
        "        }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {},
      "outputs": [],
      "source": [
        "data = [\n",
        "    {\n",
        "        'prev_tweet': \"@WholeMarsBlog Headline is misleading. Starlink can obviously offer far more robust positioning than GPS, as it will have ~1000X more satellites over time. Not all will have line of sight to users, but still >10X GPS & far stronger signal. Just not today’s problem.\",\n",
        "        'curr_tweet': \"@spideycyp_155 @BillyM2k If Russia faced calamitous defeat in conventional warfare for something as strategically critical as Crimea, the probability of using nuclear weapons is high\",\n",
        "        'prev_reddit': \"We know who controls the media. The same corporations who have wreaked havoc on the globe for decades, if not centuries, the big banks who financed them, and the governments who turned a blind eye to the destruction. The same entities who have brought us to the precipice of destruction - quite possibly condemning us, and our progeny to an unlivable climate They have tried to stop you at every turn, and yet you persist for the good of humanity. We love you, Elon! Keep up the good work! As you have said, we must never let the light of human consciousness fade - never!\",\n",
        "        'likes': 100,  \n",
        "        'retweets': 50, \n",
        "        'comments': 25  \n",
        "    },\n",
        "    {\n",
        "        'prev_tweet': \"@WholeMarsBlog Headline is misleading. Starlink can obviously offer far more robust positioning than GPS, as it will have ~1000X more satellites over time. Not all will have line of sight to users, but still >10X GPS & far stronger signal. Just not today’s problem.\",\n",
        "        'curr_tweet': \"@spideycyp_155 @BillyM2k If Russia faced calamitous defeat in conventional warfare for something as strategically critical as Crimea, the probability of using nuclear weapons is high\",\n",
        "        'prev_reddit': \"We know who controls the media. The same corporations who have wreaked havoc on the globe for decades, if not centuries, the big banks who financed them, and the governments who turned a blind eye to the destruction. The same entities who have brought us to the precipice of destruction - quite possibly condemning us, and our progeny to an unlivable climate They have tried to stop you at every turn, and yet you persist for the good of humanity. We love you, Elon! Keep up the good work! As you have said, we must never let the light of human consciousness fade - never!\",\n",
        "        'likes': 100,  \n",
        "        'retweets': 50, \n",
        "        'comments': 25  \n",
        "    },\n",
        "    {\n",
        "        'prev_tweet': \"@WholeMarsBlog Headline is misleading. Starlink can obviously offer far more robust positioning than GPS, as it will have ~1000X more satellites over time. Not all will have line of sight to users, but still >10X GPS & far stronger signal. Just not today’s problem.\",\n",
        "        'curr_tweet': \"@spideycyp_155 @BillyM2k If Russia faced calamitous defeat in conventional warfare for something as strategically critical as Crimea, the probability of using nuclear weapons is high\",\n",
        "        'prev_reddit': \"We know who controls the media. The same corporations who have wreaked havoc on the globe for decades, if not centuries, the big banks who financed them, and the governments who turned a blind eye to the destruction. The same entities who have brought us to the precipice of destruction - quite possibly condemning us, and our progeny to an unlivable climate They have tried to stop you at every turn, and yet you persist for the good of humanity. We love you, Elon! Keep up the good work! As you have said, we must never let the light of human consciousness fade - never!\",\n",
        "        'likes': 100,  \n",
        "        'retweets': 50, \n",
        "        'comments': 25  \n",
        "    },\n",
        "    {\n",
        "        'prev_tweet': \"@WholeMarsBlog Headline is misleading. Starlink can obviously offer far more robust positioning than GPS, as it will have ~1000X more satellites over time. Not all will have line of sight to users, but still >10X GPS & far stronger signal. Just not today’s problem.\",\n",
        "        'curr_tweet': \"@spideycyp_155 @BillyM2k If Russia faced calamitous defeat in conventional warfare for something as strategically critical as Crimea, the probability of using nuclear weapons is high\",\n",
        "        'prev_reddit': \"We know who controls the media. The same corporations who have wreaked havoc on the globe for decades, if not centuries, the big banks who financed them, and the governments who turned a blind eye to the destruction. The same entities who have brought us to the precipice of destruction - quite possibly condemning us, and our progeny to an unlivable climate They have tried to stop you at every turn, and yet you persist for the good of humanity. We love you, Elon! Keep up the good work! As you have said, we must never let the light of human consciousness fade - never!\",\n",
        "        'likes': 100,  \n",
        "        'retweets': 50, \n",
        "        'comments': 25  \n",
        "    }\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ],
      "source": [
        "preprocessing_model = create_preprocessing_model(\n",
        "    \"mixedbread-ai/mxbai-embed-large-v1\",\n",
        "    \"cardiffnlp/twitter-roberta-base-sentiment-latest\",\n",
        "    \"bhadresh-savani/distilbert-base-uncased-emotion\",\n",
        "    \"cardiffnlp/tweet-topic-latest-multi\"\n",
        ")\n",
        "\n",
        "dataset = DataLoaderDUNES(data, preprocessing_model)\n",
        "dataloader = DataLoader(dataset, batch_size=2, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {},
      "outputs": [],
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        pe = torch.zeros(max_len, 1, d_model)\n",
        "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.pe[:x.size(0)]\n",
        "        return self.dropout(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {},
      "outputs": [],
      "source": [
        "class TransformerModel(nn.Module):\n",
        "    def __init__(self, feature_sizes, d_model, nhead, num_encoder_layers, dim_feedforward, num_outputs):\n",
        "        super(TransformerModel, self).__init__()\n",
        "        self.positional_encoder = PositionalEncoding(d_model)\n",
        "        self.fc1 = nn.Linear(sum(feature_sizes.values()), d_model)\n",
        "        self.fc2 = nn.Linear(d_model, d_model)\n",
        "        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead, dim_feedforward=dim_feedforward)\n",
        "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_encoder_layers)\n",
        "        self.output_linear = nn.Linear(d_model, num_outputs)\n",
        "\n",
        "    def forward(self, features):\n",
        "        # Concatenation of all features to create the input tensor\n",
        "        src = torch.cat([features[key] for key in features], dim=1)\n",
        "        # print(src.shape)\n",
        "        src = self.fc1(src)\n",
        "        src = self.fc2(src)\n",
        "        src = self.positional_encoder(src)\n",
        "        \n",
        "        # Transformer encoding and output processing remain unchanged\n",
        "        output = self.transformer_encoder(src)\n",
        "        output = output.mean(dim=0)\n",
        "        output = self.output_linear(output)\n",
        "        \n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = TransformerModel(\n",
        "    feature_sizes={\n",
        "        'tweet_embedding': preprocessing_model.feature_size['twitter_embedding'],  # Size of the tweet embeddings\n",
        "        'tweet_sentiment': preprocessing_model.feature_size['twitter_sentiment'],  # Size of the tweet sentiment vector\n",
        "        'reddit_sentiment': preprocessing_model.feature_size['reddit_sentiment'],  # Size of the Reddit sentiment vector\n",
        "        'tweet_sector': preprocessing_model.feature_size['twitter_sector'],  # Size of the tweet sector vector\n",
        "    },\n",
        "    d_model=1024,  # Size of each projection layer\n",
        "    nhead=8,  # Number of attention heads in the transformer encoder\n",
        "    num_encoder_layers=3,  # Number of layers in the transformer encoder\n",
        "    dim_feedforward=2048,  # Size of the feedforward network model in transformer encoder\n",
        "    num_outputs=3  # Number of output values (e.g., predicting engagement metrics)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "fc1.weight: torch.Size([1024, 1052])\n",
            "fc1.bias: torch.Size([1024])\n",
            "fc2.weight: torch.Size([1024, 1024])\n",
            "fc2.bias: torch.Size([1024])\n",
            "transformer_encoder.layers.0.self_attn.in_proj_weight: torch.Size([3072, 1024])\n",
            "transformer_encoder.layers.0.self_attn.in_proj_bias: torch.Size([3072])\n",
            "transformer_encoder.layers.0.self_attn.out_proj.weight: torch.Size([1024, 1024])\n",
            "transformer_encoder.layers.0.self_attn.out_proj.bias: torch.Size([1024])\n",
            "transformer_encoder.layers.0.linear1.weight: torch.Size([2048, 1024])\n",
            "transformer_encoder.layers.0.linear1.bias: torch.Size([2048])\n",
            "transformer_encoder.layers.0.linear2.weight: torch.Size([1024, 2048])\n",
            "transformer_encoder.layers.0.linear2.bias: torch.Size([1024])\n",
            "transformer_encoder.layers.0.norm1.weight: torch.Size([1024])\n",
            "transformer_encoder.layers.0.norm1.bias: torch.Size([1024])\n",
            "transformer_encoder.layers.0.norm2.weight: torch.Size([1024])\n",
            "transformer_encoder.layers.0.norm2.bias: torch.Size([1024])\n",
            "transformer_encoder.layers.1.self_attn.in_proj_weight: torch.Size([3072, 1024])\n",
            "transformer_encoder.layers.1.self_attn.in_proj_bias: torch.Size([3072])\n",
            "transformer_encoder.layers.1.self_attn.out_proj.weight: torch.Size([1024, 1024])\n",
            "transformer_encoder.layers.1.self_attn.out_proj.bias: torch.Size([1024])\n",
            "transformer_encoder.layers.1.linear1.weight: torch.Size([2048, 1024])\n",
            "transformer_encoder.layers.1.linear1.bias: torch.Size([2048])\n",
            "transformer_encoder.layers.1.linear2.weight: torch.Size([1024, 2048])\n",
            "transformer_encoder.layers.1.linear2.bias: torch.Size([1024])\n",
            "transformer_encoder.layers.1.norm1.weight: torch.Size([1024])\n",
            "transformer_encoder.layers.1.norm1.bias: torch.Size([1024])\n",
            "transformer_encoder.layers.1.norm2.weight: torch.Size([1024])\n",
            "transformer_encoder.layers.1.norm2.bias: torch.Size([1024])\n",
            "transformer_encoder.layers.2.self_attn.in_proj_weight: torch.Size([3072, 1024])\n",
            "transformer_encoder.layers.2.self_attn.in_proj_bias: torch.Size([3072])\n",
            "transformer_encoder.layers.2.self_attn.out_proj.weight: torch.Size([1024, 1024])\n",
            "transformer_encoder.layers.2.self_attn.out_proj.bias: torch.Size([1024])\n",
            "transformer_encoder.layers.2.linear1.weight: torch.Size([2048, 1024])\n",
            "transformer_encoder.layers.2.linear1.bias: torch.Size([2048])\n",
            "transformer_encoder.layers.2.linear2.weight: torch.Size([1024, 2048])\n",
            "transformer_encoder.layers.2.linear2.bias: torch.Size([1024])\n",
            "transformer_encoder.layers.2.norm1.weight: torch.Size([1024])\n",
            "transformer_encoder.layers.2.norm1.bias: torch.Size([1024])\n",
            "transformer_encoder.layers.2.norm2.weight: torch.Size([1024])\n",
            "transformer_encoder.layers.2.norm2.bias: torch.Size([1024])\n",
            "output_linear.weight: torch.Size([3, 1024])\n",
            "output_linear.bias: torch.Size([3])\n"
          ]
        }
      ],
      "source": [
        "for name, param in model.named_parameters():\n",
        "    print(f\"{name}: {param.size()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {},
      "outputs": [],
      "source": [
        "criterion = nn.MSELoss()\n",
        "optimizer = Adam(model.parameters(), lr=0.001)\n",
        "num_epochs = 25"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/25, Train Loss: 1331.9509\n",
            "Epoch 2/25, Train Loss: 1230.4149\n",
            "Epoch 3/25, Train Loss: 1135.3431\n",
            "Epoch 4/25, Train Loss: 1042.3410\n",
            "Epoch 5/25, Train Loss: 957.8941\n",
            "Epoch 6/25, Train Loss: 876.0158\n",
            "Epoch 7/25, Train Loss: 796.9320\n",
            "Epoch 8/25, Train Loss: 724.8004\n",
            "Epoch 9/25, Train Loss: 660.7253\n",
            "Epoch 10/25, Train Loss: 597.7673\n",
            "Epoch 11/25, Train Loss: 539.6107\n",
            "Epoch 12/25, Train Loss: 484.6129\n",
            "Epoch 13/25, Train Loss: 435.4248\n",
            "Epoch 14/25, Train Loss: 388.5339\n",
            "Epoch 15/25, Train Loss: 348.1073\n",
            "Epoch 16/25, Train Loss: 307.4407\n",
            "Epoch 17/25, Train Loss: 274.0720\n",
            "Epoch 18/25, Train Loss: 241.2054\n",
            "Epoch 19/25, Train Loss: 212.2733\n",
            "Epoch 20/25, Train Loss: 185.1771\n",
            "Epoch 21/25, Train Loss: 160.6241\n",
            "Epoch 22/25, Train Loss: 139.8774\n",
            "Epoch 23/25, Train Loss: 119.3482\n",
            "Epoch 24/25, Train Loss: 103.4539\n",
            "Epoch 25/25, Train Loss: 87.4164\n"
          ]
        }
      ],
      "source": [
        "for epoch in range(num_epochs):\n",
        "    # Training phase\n",
        "    model.train()  \n",
        "    train_loss = 0.0\n",
        "    for batch in dataloader:\n",
        "        optimizer.zero_grad()\n",
        "        features = {\n",
        "            'prev_tweet_embedding': batch['prev_tweet_embedding'],\n",
        "            'prev_tweet_sentiment': batch['prev_tweet_sentiment'],\n",
        "            'prev_reddit_sentiment': batch['prev_reddit_sentiment'],\n",
        "            'prev_tweet_sector': batch['prev_tweet_sector'],\n",
        "        }\n",
        "        targets = torch.stack((batch['likes'], batch['retweets'], batch['comments']), dim=1)\n",
        "        outputs = model(features)\n",
        "        loss = criterion(outputs, targets)\n",
        "        train_loss += loss.item()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    avg_train_loss = train_loss / len(dataloader)\n",
        "    print(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {avg_train_loss:.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prediction phase\n",
        "model.eval()  # Set the model to evaluation mode\n",
        "with torch.no_grad():  # Disable gradient calculation\n",
        "    total_predictions = []\n",
        "    total_targets = []\n",
        "    for batch in validation_dataloader:\n",
        "        features = {\n",
        "            'prev_tweet_embedding': batch['prev_tweet_embedding'],\n",
        "            'prev_tweet_sentiment': batch['prev_tweet_sentiment'],\n",
        "            'prev_reddit_sentiment': batch['prev_reddit_sentiment'],\n",
        "            'prev_tweet_sector': batch['prev_tweet_sector'],\n",
        "        }\n",
        "        targets = torch.stack((batch['likes'], batch['retweets'], batch['comments']), dim=1)\n",
        "        predictions = model(features)\n",
        "        total_predictions.append(predictions)\n",
        "        total_targets.append(targets)\n",
        "    \n",
        "    # Conversion to numpy for analysis (optional)\n",
        "    predictions_np = torch.cat(total_predictions, dim=0).cpu().numpy()\n",
        "    targets_np = torch.cat(total_targets, dim=0).cpu().numpy()\n",
        "    # Use predictions_np and targets_np for further analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

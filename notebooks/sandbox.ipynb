{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### download transformers & librariees"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kUN3F-XE1dqr",
        "outputId": "38d5f67f-aa91-48bc-bbc6-a0b7e1cc0450"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: sentence-transformers in c:\\users\\nikit\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (2.6.1)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.32.0 in c:\\users\\nikit\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sentence-transformers) (4.39.1)\n",
            "Requirement already satisfied: tqdm in c:\\users\\nikit\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sentence-transformers) (4.66.2)\n",
            "Requirement already satisfied: torch>=1.11.0 in c:\\users\\nikit\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sentence-transformers) (2.2.1)\n",
            "Requirement already satisfied: numpy in c:\\users\\nikit\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sentence-transformers) (1.26.4)\n",
            "Requirement already satisfied: scikit-learn in c:\\users\\nikit\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sentence-transformers) (1.4.1.post1)\n",
            "Requirement already satisfied: scipy in c:\\users\\nikit\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sentence-transformers) (1.12.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.15.1 in c:\\users\\nikit\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sentence-transformers) (0.22.1)\n",
            "Requirement already satisfied: Pillow in c:\\users\\nikit\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sentence-transformers) (10.2.0)\n",
            "Requirement already satisfied: filelock in c:\\users\\nikit\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (3.13.3)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\nikit\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (2024.3.1)\n",
            "Requirement already satisfied: packaging>=20.9 in c:\\users\\nikit\\appdata\\roaming\\python\\python310\\site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\nikit\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (6.0.1)\n",
            "Requirement already satisfied: requests in c:\\users\\nikit\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (2.31.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\nikit\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (4.10.0)\n",
            "Requirement already satisfied: sympy in c:\\users\\nikit\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (1.12)\n",
            "Requirement already satisfied: networkx in c:\\users\\nikit\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in c:\\users\\nikit\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.1.3)\n",
            "Requirement already satisfied: colorama in c:\\users\\nikit\\appdata\\roaming\\python\\python310\\site-packages (from tqdm->sentence-transformers) (0.4.6)\n",
            "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\nikit\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers<5.0.0,>=4.32.0->sentence-transformers) (2023.12.25)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in c:\\users\\nikit\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers<5.0.0,>=4.32.0->sentence-transformers) (0.15.2)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\nikit\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers<5.0.0,>=4.32.0->sentence-transformers) (0.4.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\nikit\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-learn->sentence-transformers) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\nikit\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-learn->sentence-transformers) (3.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\nikit\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\nikit\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\nikit\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\nikit\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (2.2.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\nikit\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (2024.2.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in c:\\users\\nikit\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!python -m pip install -U sentence-transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "aKCF9Q56ue-G"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\nikit\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from typing import Dict\n",
        "from scipy.special import expit, softmax\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import AutoModel, AutoTokenizer, AutoModelForSequenceClassification\n",
        "from sentence_transformers import SentenceTransformer\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### preprocessing experimentation\n",
        "The PreDUNES class serves as a PyTorch module for preprocessing textual data from Twitter and Reddit. Upon initialization, it receives various components including models and tokenizers for generating embeddings, predicting sentiment, and classifying sector information from Twitter and Reddit text. Its forward method executes the preprocessing steps, involving the extraction of embeddings, sentiment analysis, and sector classification for both the previous and current tweets, along with sentiment analysis for the previous Reddit post. The create_preprocessing_model function facilitates the initialization of the preprocessing module by loading the required models and setting them to evaluation mode, ensuring their parameters are frozen for training. This modular approach streamlines the setup of the preprocessing pipeline for downstream tasks such as sentiment analysis and sector classification on textual data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "mmpWO07qaY1q"
      },
      "outputs": [],
      "source": [
        "class PreDUNES(nn.Module):\n",
        "    def __init__(\n",
        "            self,\n",
        "            twitter_embedding_model: nn.Module,\n",
        "            twitter_sentiment_tokenizer,\n",
        "            twitter_sentiment_model: nn.Module,\n",
        "            reddit_sentiment_tokenizer,\n",
        "            reddit_sentiment_model: nn.Module,\n",
        "            twitter_sector_tokenizer,\n",
        "            twitter_sector_model: nn.Module\n",
        "        ):\n",
        "        '''\n",
        "        Initialize a DUNES model from a set of embeddings and sentiment models.\n",
        "        Args:\n",
        "            twitter_embedding_model: huggingface model for Twitter embeddings\n",
        "            twitter_sentiment_model: huggingface model for Twitter sentiment\n",
        "            reddit_sentiment_model: huggingface model for Reddit sentiment\n",
        "            twitter_sector_model: huggingface model for Twitter sector\n",
        "        '''\n",
        "        super(PreDUNES, self).__init__()\n",
        "        self.twitter_embedding_model = twitter_embedding_model\n",
        "        self.twitter_sentiment_tokenizer = twitter_sentiment_tokenizer\n",
        "        self.twitter_sentiment_model = twitter_sentiment_model\n",
        "        self.reddit_sentiment_tokenizer = reddit_sentiment_tokenizer\n",
        "        self.reddit_sentiment_model = reddit_sentiment_model\n",
        "        self.twitter_sector_tokenizer = twitter_sector_tokenizer\n",
        "        self.twitter_sector_model = twitter_sector_model\n",
        "\n",
        "    def forward(self, prev_tweet, curr_tweet, prev_reddit):\n",
        "        '''\n",
        "        Forward pass for the DUNES model.\n",
        "        Args:\n",
        "            prev_tweet: previous tweet\n",
        "            curr_tweet: current tweet\n",
        "            prev_reddit: previous Reddit post\n",
        "            curr_reddit: current Reddit post\n",
        "        Returns:\n",
        "            sentiment: sentiment of the current tweet\n",
        "            sector: sector of the current tweet\n",
        "        '''\n",
        "        # Get the embeddings\n",
        "        prev_tweet_embedding = self.twitter_embedding_model.encode([prev_tweet])\n",
        "        curr_tweet_embedding = self.twitter_embedding_model.encode([curr_tweet])\n",
        "\n",
        "        # Get the sentiment\n",
        "        prev_tweet_tokens = self.twitter_sentiment_tokenizer(prev_tweet, return_tensors='pt')\n",
        "        prev_tweet_sentiment = self.twitter_sentiment_model(**prev_tweet_tokens)[0][0].detach().numpy()\n",
        "        curr_tweet_tokens = self.twitter_sentiment_tokenizer(curr_tweet, return_tensors='pt')\n",
        "        curr_tweet_sentiment = self.twitter_sentiment_model(**curr_tweet_tokens)[0][0].detach().numpy()\n",
        "\n",
        "        prev_reddit_tokens = self.reddit_sentiment_tokenizer(prev_reddit, return_tensors='pt')\n",
        "        prev_reddit_sentiment = self.reddit_sentiment_model(**prev_reddit_tokens)[0][0].detach().numpy()\n",
        "\n",
        "        # Get the sector\n",
        "        prev_sector_tokens = self.twitter_sector_tokenizer(prev_tweet, return_tensors='pt')\n",
        "        prev_tweet_sector = self.twitter_sector_model(**prev_sector_tokens)[0][0].detach().numpy()\n",
        "        curr_sector_tokens = self.twitter_sector_tokenizer(curr_tweet, return_tensors='pt')\n",
        "        curr_tweet_sector = self.twitter_sector_model(**curr_sector_tokens)[0][0].detach().numpy()\n",
        "\n",
        "\n",
        "        return prev_tweet_embedding, curr_tweet_embedding, prev_tweet_sentiment, curr_tweet_sentiment, prev_reddit_sentiment, prev_tweet_sector, curr_tweet_sector\n",
        "\n",
        "\n",
        "def create_preprocessing_model(\n",
        "        twitter_embedding: str,\n",
        "        twitter_sentiment: str,\n",
        "        reddit_sentiment: str,\n",
        "        twitter_sector: str\n",
        "):\n",
        "    '''\n",
        "    Initialize a DUNES model from a set of embeddings and sentiment models.\n",
        "    Feeds the output of these models to a transformer for classification.\n",
        "    Args:\n",
        "        twitter_embedding: path to the Twitter embedding huggingface model\n",
        "        twitter_sentiment: path to the Twitter sentiment huggingface model\n",
        "        reddit_sentiment: path to the Reddit sentiment huggingface model\n",
        "        twitter_sector: path to the Twitter sector huggingface model\n",
        "    '''\n",
        "\n",
        "    # Load the models\n",
        "    twitter_embedding_model = SentenceTransformer(twitter_embedding)\n",
        "    twitter_sentiment_tokenizer = AutoTokenizer.from_pretrained(twitter_sentiment)\n",
        "    twitter_sentiment_model = AutoModelForSequenceClassification.from_pretrained(twitter_sentiment)\n",
        "    reddit_sentiment_tokenizer = AutoTokenizer.from_pretrained(reddit_sentiment)\n",
        "    reddit_sentiment_model = AutoModelForSequenceClassification.from_pretrained(reddit_sentiment)\n",
        "    twitter_sector_tokenizer = AutoTokenizer.from_pretrained(twitter_sector)\n",
        "    twitter_sector_model = AutoModelForSequenceClassification.from_pretrained(twitter_sector)\n",
        "\n",
        "    # Freeze the models\n",
        "    twitter_sentiment_model.eval()\n",
        "    twitter_sentiment_model.requires_grad_(False)\n",
        "    reddit_sentiment_model.eval()\n",
        "    reddit_sentiment_model.requires_grad_(False)\n",
        "    twitter_sector_model.eval()\n",
        "    twitter_sector_model.requires_grad_(False)\n",
        "\n",
        "    # Create the DUNES model\n",
        "    model = PreDUNES(\n",
        "        twitter_embedding_model,\n",
        "        twitter_sentiment_tokenizer,\n",
        "        twitter_sentiment_model,\n",
        "        reddit_sentiment_tokenizer,\n",
        "        reddit_sentiment_model,\n",
        "        twitter_sector_tokenizer,\n",
        "        twitter_sector_model\n",
        "        )\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "class PreDUNES(nn.Module):\n",
        "    def __init__(\n",
        "            self,\n",
        "            twitter_embedding_model: nn.Module,\n",
        "            twitter_sentiment_tokenizer,\n",
        "            twitter_sentiment_model: nn.Module,\n",
        "            reddit_sentiment_tokenizer,\n",
        "            reddit_sentiment_model: nn.Module,\n",
        "            twitter_sector_tokenizer,\n",
        "            twitter_sector_model: nn.Module\n",
        "        ):\n",
        "        '''\n",
        "        Initialize a DUNES model from a set of embeddings and sentiment models.\n",
        "        Args:\n",
        "            twitter_embedding_model: huggingface model for Twitter embeddings\n",
        "            twitter_sentiment_model: huggingface model for Twitter sentiment\n",
        "            reddit_sentiment_model: huggingface model for Reddit sentiment\n",
        "            twitter_sector_model: huggingface model for Twitter sector\n",
        "        '''\n",
        "        super(PreDUNES, self).__init__()\n",
        "        self.twitter_embedding_model = twitter_embedding_model\n",
        "        self.twitter_sentiment_tokenizer = twitter_sentiment_tokenizer\n",
        "        self.twitter_sentiment_model = twitter_sentiment_model\n",
        "        self.reddit_sentiment_tokenizer = reddit_sentiment_tokenizer\n",
        "        self.reddit_sentiment_model = reddit_sentiment_model\n",
        "        self.twitter_sector_tokenizer = twitter_sector_tokenizer\n",
        "        self.twitter_sector_model = twitter_sector_model\n",
        "\n",
        "    def forward(self, prev_tweet, curr_tweet, prev_reddit):\n",
        "        '''\n",
        "        Forward pass for the DUNES model.\n",
        "        Args:\n",
        "            prev_tweet: previous tweet\n",
        "            curr_tweet: current tweet\n",
        "            prev_reddit: previous Reddit post\n",
        "            curr_reddit: current Reddit post\n",
        "        Returns:\n",
        "            sentiment: sentiment of the current tweet\n",
        "            sector: sector of the current tweet\n",
        "        '''\n",
        "        # Get the embeddings\n",
        "        prev_tweet_embedding = self.twitter_embedding_model.encode([prev_tweet])\n",
        "        curr_tweet_embedding = self.twitter_embedding_model.encode([curr_tweet])\n",
        "\n",
        "        # Get the sentiment\n",
        "        prev_tweet_tokens = self.twitter_sentiment_tokenizer(prev_tweet, return_tensors='pt')\n",
        "        prev_tweet_sentiment = self.twitter_sentiment_model(**prev_tweet_tokens)[0][0].detach().numpy()\n",
        "        curr_tweet_tokens = self.twitter_sentiment_tokenizer(curr_tweet, return_tensors='pt')\n",
        "        curr_tweet_sentiment = self.twitter_sentiment_model(**curr_tweet_tokens)[0][0].detach().numpy()\n",
        "\n",
        "        prev_reddit_tokens = self.reddit_sentiment_tokenizer(prev_reddit, return_tensors='pt')\n",
        "        prev_reddit_sentiment = self.reddit_sentiment_model(**prev_reddit_tokens)[0][0].detach().numpy()\n",
        "\n",
        "        # Get the sector\n",
        "        prev_sector_tokens = self.twitter_sector_tokenizer(prev_tweet, return_tensors='pt')\n",
        "        prev_tweet_sector = self.twitter_sector_model(**prev_sector_tokens)[0][0].detach().numpy()\n",
        "        curr_sector_tokens = self.twitter_sector_tokenizer(curr_tweet, return_tensors='pt')\n",
        "        curr_tweet_sector = self.twitter_sector_model(**curr_sector_tokens)[0][0].detach().numpy()\n",
        "\n",
        "\n",
        "        return prev_tweet_embedding, curr_tweet_embedding, prev_tweet_sentiment, curr_tweet_sentiment, prev_reddit_sentiment, prev_tweet_sector, curr_tweet_sector\n",
        "\n",
        "\n",
        "def create_preprocessing_model(\n",
        "        twitter_embedding: str,\n",
        "        twitter_sentiment: str,\n",
        "        reddit_sentiment: str,\n",
        "        twitter_sector: str\n",
        "):\n",
        "    '''\n",
        "    Initialize a DUNES model from a set of embeddings and sentiment models.\n",
        "    Feeds the output of these models to a transformer for classification.\n",
        "    Args:\n",
        "        twitter_embedding: path to the Twitter embedding huggingface model\n",
        "        twitter_sentiment: path to the Twitter sentiment huggingface model\n",
        "        reddit_sentiment: path to the Reddit sentiment huggingface model\n",
        "        twitter_sector: path to the Twitter sector huggingface model\n",
        "    '''\n",
        "\n",
        "    # Load the models\n",
        "    twitter_embedding_model = SentenceTransformer(twitter_embedding)\n",
        "    twitter_sentiment_tokenizer = AutoTokenizer.from_pretrained(twitter_sentiment)\n",
        "    twitter_sentiment_model = AutoModelForSequenceClassification.from_pretrained(twitter_sentiment)\n",
        "    reddit_sentiment_tokenizer = AutoTokenizer.from_pretrained(reddit_sentiment)\n",
        "    reddit_sentiment_model = AutoModelForSequenceClassification.from_pretrained(reddit_sentiment)\n",
        "    twitter_sector_tokenizer = AutoTokenizer.from_pretrained(twitter_sector)\n",
        "    twitter_sector_model = AutoModelForSequenceClassification.from_pretrained(twitter_sector)\n",
        "\n",
        "    # Freeze the models\n",
        "    twitter_sentiment_model.eval()\n",
        "    twitter_sentiment_model.requires_grad_(False)\n",
        "    reddit_sentiment_model.eval()\n",
        "    reddit_sentiment_model.requires_grad_(False)\n",
        "    twitter_sector_model.eval()\n",
        "    twitter_sector_model.requires_grad_(False)\n",
        "\n",
        "    # Create the DUNES model\n",
        "    model = PreDUNES(\n",
        "        twitter_embedding_model,\n",
        "        twitter_sentiment_tokenizer,\n",
        "        twitter_sentiment_model,\n",
        "        reddit_sentiment_tokenizer,\n",
        "        reddit_sentiment_model,\n",
        "        twitter_sector_tokenizer,\n",
        "        twitter_sector_model\n",
        "        )\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C78OGHLymXEh",
        "outputId": "b2ee7cbf-1973-450a-a94d-7461e665bc63"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ],
      "source": [
        "preprocessing_model = create_preprocessing_model(\n",
        "    \"mixedbread-ai/mxbai-embed-large-v1\",\n",
        "    \"cardiffnlp/twitter-roberta-base-sentiment-latest\",\n",
        "    \"SamLowe/roberta-base-go_emotions\",\n",
        "    \"cardiffnlp/tweet-topic-latest-multi\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "axcbDtf-n6h_"
      },
      "outputs": [],
      "source": [
        "prev_tweet_embedding, curr_tweet_embedding, prev_tweet_sentiment, curr_tweet_sentiment, prev_reddit_sentiment, prev_tweet_sector, curr_tweet_sector = preprocessing_model(\n",
        "    \"@WholeMarsBlog Headline is misleading. Starlink can obviously offer far more robust positioning than GPS, as it will have ~1000X more satellites over time. Not all will have line of sight to users, but still &gt;10X GPS &amp; far stronger signal. Just not today’s problem.\",\n",
        "    \"@spideycyp_155 @BillyM2k If Russia faced calamitous defeat in conventional warfare for something as strategically critical as Crimea, the probability of using nuclear weapons is high\",\n",
        "    \"We know who controls the media. The same corporations who have wreaked havoc on the globe for decades, if not centuries, the big banks who financed them, and the governments who turned a blind eye to the destruction. The same entities who have brought us to the precipice of destruction - quite possibly condemning us, and our progeny to an unlivable climate They have tried to stop you at every turn, and yet you persist for the good of humanity. We love you, Elon! Keep up the good work! As you have said, we must never let the light of human consciousness fade - never!\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "AB2dLEAbrJeX"
      },
      "outputs": [],
      "source": [
        "predictions = softmax(prev_reddit_sentiment)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "080hMwxmOoLv",
        "outputId": "68b90f75-b051-4b3e-e645-4338271f5990"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at SamLowe/roberta-base-go_emotions and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1) admiration 0.5544\n",
            "2) love 0.2888\n",
            "3) approval 0.0619\n",
            "4) caring 0.023\n",
            "5) neutral 0.0127\n",
            "6) optimism 0.0125\n",
            "7) disapproval 0.0091\n",
            "8) annoyance 0.0063\n",
            "9) anger 0.0055\n",
            "10) disappointment 0.004\n",
            "11) gratitude 0.0036\n",
            "12) desire 0.0036\n",
            "13) realization 0.0026\n",
            "14) pride 0.0022\n",
            "15) sadness 0.0021\n",
            "16) joy 0.0013\n",
            "17) excitement 0.0011\n",
            "18) disgust 0.001\n",
            "19) fear 0.0007\n",
            "20) relief 0.0006\n",
            "21) confusion 0.0006\n",
            "22) grief 0.0005\n",
            "23) remorse 0.0005\n",
            "24) curiosity 0.0004\n",
            "25) surprise 0.0003\n",
            "26) nervousness 0.0003\n",
            "27) embarrassment 0.0002\n",
            "28) amusement 0.0001\n"
          ]
        }
      ],
      "source": [
        "class_mapping = AutoModel.from_pretrained(\"SamLowe/roberta-base-go_emotions\").config.id2label\n",
        "\n",
        "ranking = np.argsort(predictions)\n",
        "ranking = ranking[::-1]\n",
        "for i in range(predictions.shape[0]):\n",
        "    l = class_mapping[ranking[i]]\n",
        "    s = predictions[ranking[i]]\n",
        "    print(f\"{i+1}) {l} {np.round(float(s), 4)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### real pre-processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "xVTxTCOfO7Yi"
      },
      "outputs": [],
      "source": [
        "class PreDUNES(nn.Module):\n",
        "    def __init__(\n",
        "            self,\n",
        "            twitter_embedding_model: nn.Module,\n",
        "            twitter_sentiment_tokenizer,\n",
        "            twitter_sentiment_model: nn.Module,\n",
        "            reddit_sentiment_tokenizer,\n",
        "            reddit_sentiment_model: nn.Module,\n",
        "            twitter_sector_tokenizer,\n",
        "            twitter_sector_model: nn.Module\n",
        "        ):\n",
        "        '''\n",
        "        Initialize a DUNES model from a set of embeddings and sentiment models.\n",
        "        Args:\n",
        "            twitter_embedding_model: huggingface model for Twitter embeddings\n",
        "            twitter_sentiment_model: huggingface model for Twitter sentiment\n",
        "            reddit_sentiment_model: huggingface model for Reddit sentiment\n",
        "            twitter_sector_model: huggingface model for Twitter sector\n",
        "        '''\n",
        "        super(PreDUNES, self).__init__()\n",
        "        self.twitter_embedding_model = twitter_embedding_model\n",
        "        self.twitter_sentiment_tokenizer = twitter_sentiment_tokenizer\n",
        "        self.twitter_sentiment_model = twitter_sentiment_model\n",
        "        self.reddit_sentiment_tokenizer = reddit_sentiment_tokenizer\n",
        "        self.reddit_sentiment_model = reddit_sentiment_model\n",
        "        self.twitter_sector_tokenizer = twitter_sector_tokenizer\n",
        "        self.twitter_sector_model = twitter_sector_model\n",
        "\n",
        "    def forward(self, prev_tweet, curr_tweet, prev_reddit):\n",
        "        '''\n",
        "        Forward pass for the DUNES model.\n",
        "        Args:\n",
        "            prev_tweet: previous tweet\n",
        "            curr_tweet: current tweet\n",
        "            prev_reddit: previous Reddit post\n",
        "            curr_reddit: current Reddit post\n",
        "        Returns:\n",
        "            sentiment: sentiment of the current tweet\n",
        "            sector: sector of the current tweet\n",
        "        '''\n",
        "        # Get the embeddings\n",
        "        prev_tweet_embedding = self.twitter_embedding_model.encode([prev_tweet])\n",
        "        curr_tweet_embedding = self.twitter_embedding_model.encode([curr_tweet])\n",
        "\n",
        "        # Get the sentiment\n",
        "        prev_tweet_tokens = self.twitter_sentiment_tokenizer(prev_tweet, return_tensors='pt')\n",
        "        prev_tweet_sentiment = self.twitter_sentiment_model(**prev_tweet_tokens)[0][0].detach().numpy()\n",
        "        curr_tweet_tokens = self.twitter_sentiment_tokenizer(curr_tweet, return_tensors='pt')\n",
        "        curr_tweet_sentiment = self.twitter_sentiment_model(**curr_tweet_tokens)[0][0].detach().numpy()\n",
        "\n",
        "        prev_reddit_tokens = self.reddit_sentiment_tokenizer(prev_reddit, return_tensors='pt')\n",
        "        prev_reddit_sentiment = self.reddit_sentiment_model(**prev_reddit_tokens)[0][0].detach().numpy()\n",
        "\n",
        "        # Get the sector\n",
        "        prev_sector_tokens = self.twitter_sector_tokenizer(prev_tweet, return_tensors='pt')\n",
        "        prev_tweet_sector = self.twitter_sector_model(**prev_sector_tokens)[0][0].detach().numpy()\n",
        "        curr_sector_tokens = self.twitter_sector_tokenizer(curr_tweet, return_tensors='pt')\n",
        "        curr_tweet_sector = self.twitter_sector_model(**curr_sector_tokens)[0][0].detach().numpy()\n",
        "\n",
        "\n",
        "        return prev_tweet_embedding, curr_tweet_embedding, prev_tweet_sentiment, curr_tweet_sentiment, prev_reddit_sentiment, prev_tweet_sector, curr_tweet_sector\n",
        "\n",
        "\n",
        "def create_preprocessing_model(\n",
        "        twitter_embedding: str,\n",
        "        twitter_sentiment: str,\n",
        "        reddit_sentiment: str,\n",
        "        twitter_sector: str\n",
        "):\n",
        "    '''\n",
        "    Initialize a DUNES model from a set of embeddings and sentiment models.\n",
        "    Feeds the output of these models to a transformer for classification.\n",
        "    Args:\n",
        "        twitter_embedding: path to the Twitter embedding huggingface model\n",
        "        twitter_sentiment: path to the Twitter sentiment huggingface model\n",
        "        reddit_sentiment: path to the Reddit sentiment huggingface model\n",
        "        twitter_sector: path to the Twitter sector huggingface model\n",
        "    '''\n",
        "\n",
        "    # Load the models\n",
        "    twitter_embedding_model = SentenceTransformer(twitter_embedding)\n",
        "    twitter_sentiment_tokenizer = AutoTokenizer.from_pretrained(twitter_sentiment)\n",
        "    twitter_sentiment_model = AutoModelForSequenceClassification.from_pretrained(twitter_sentiment)\n",
        "    reddit_sentiment_tokenizer = AutoTokenizer.from_pretrained(reddit_sentiment)\n",
        "    reddit_sentiment_model = AutoModelForSequenceClassification.from_pretrained(reddit_sentiment)\n",
        "    twitter_sector_tokenizer = AutoTokenizer.from_pretrained(twitter_sector)\n",
        "    twitter_sector_model = AutoModelForSequenceClassification.from_pretrained(twitter_sector)\n",
        "\n",
        "    # Freeze the models\n",
        "    twitter_sentiment_model.eval()\n",
        "    twitter_sentiment_model.requires_grad_(False)\n",
        "    reddit_sentiment_model.eval()\n",
        "    reddit_sentiment_model.requires_grad_(False)\n",
        "    twitter_sector_model.eval()\n",
        "    twitter_sector_model.requires_grad_(False)\n",
        "\n",
        "    # Create the DUNES model\n",
        "    model = PreDUNES(\n",
        "        twitter_embedding_model,\n",
        "        twitter_sentiment_tokenizer,\n",
        "        twitter_sentiment_model,\n",
        "        reddit_sentiment_tokenizer,\n",
        "        reddit_sentiment_model,\n",
        "        twitter_sector_tokenizer,\n",
        "        twitter_sector_model\n",
        "        )\n",
        "\n",
        "    return model\n",
        "\n",
        "def printClassMappings(model, predictions):\n",
        "    class_mapping = AutoModel.from_pretrained(model).config.id2label\n",
        "    predictions = softmax(predictions)\n",
        "    ranking = np.argsort(predictions)\n",
        "    ranking = ranking[::-1]\n",
        "    for i in range(predictions.shape[0]):\n",
        "        l = class_mapping[ranking[i]]\n",
        "        s = predictions[ranking[i]]\n",
        "        print(f\"{i+1}) {l} {np.round(float(s), 4)}\")\n",
        "\n",
        "def test():\n",
        "    preprocessing_model = create_preprocessing_model(\n",
        "        \"mixedbread-ai/mxbai-embed-large-v1\",\n",
        "        \"cardiffnlp/twitter-roberta-base-sentiment-latest\",\n",
        "        \"SamLowe/roberta-base-go_emotions\",\n",
        "        \"cardiffnlp/tweet-topic-latest-multi\"\n",
        "    )\n",
        "\n",
        "    prev_tweet_embedding, curr_tweet_embedding, prev_tweet_sentiment, curr_tweet_sentiment, prev_reddit_sentiment, prev_tweet_sector, curr_tweet_sector = preprocessing_model(\n",
        "        \"@WholeMarsBlog Headline is misleading. Starlink can obviously offer far more robust positioning than GPS, as it will have ~1000X more satellites over time. Not all will have line of sight to users, but still &gt;10X GPS &amp; far stronger signal. Just not today’s problem.\",\n",
        "        \"@spideycyp_155 @BillyM2k If Russia faced calamitous defeat in conventional warfare for something as strategically critical as Crimea, the probability of using nuclear weapons is high\",\n",
        "        \"We know who controls the media. The same corporations who have wreaked havoc on the globe for decades, if not centuries, the big banks who financed them, and the governments who turned a blind eye to the destruction. The same entities who have brought us to the precipice of destruction - quite possibly condemning us, and our progeny to an unlivable climate They have tried to stop you at every turn, and yet you persist for the good of humanity. We love you, Elon! Keep up the good work! As you have said, we must never let the light of human consciousness fade - never!\"\n",
        "    )\n",
        "\n",
        "    print(\"prev_tweet_embedding:\", prev_tweet_embedding)\n",
        "    print(\"curr_tweet_embedding:\", curr_tweet_embedding)\n",
        "    print(\"prev_tweet_sentiment:\", softmax(prev_tweet_sentiment))\n",
        "    print(\"curr_tweet_sentiment:\", softmax(curr_tweet_sentiment))\n",
        "\n",
        "    print(\"prev_reddit_sentiment:\")\n",
        "    printClassMappings(\"SamLowe/roberta-base-go_emotions\", prev_reddit_sentiment)\n",
        "    print(\"prev_tweet_sector:\")\n",
        "    printClassMappings(\"cardiffnlp/tweet-topic-latest-multi\", prev_tweet_sector)\n",
        "    print(\"curr_tweet_sector:\")\n",
        "    printClassMappings(\"cardiffnlp/tweet-topic-latest-multi\", curr_tweet_sector)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gEQrY1lLW0bb",
        "outputId": "8eb7baa3-ecd1-41c2-fba1-802c6cfd551b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "prev_tweet_embedding: [[ 0.34489498  0.13846147 -0.32645515 ... -0.7176758   0.28787854\n",
            "  -0.14000645]]\n",
            "curr_tweet_embedding: [[ 0.66635376 -0.49894774  0.04492863 ... -0.9171951   0.18333018\n",
            "  -0.4950185 ]]\n",
            "prev_tweet_sentiment: [0.14040028 0.57469916 0.28490052]\n",
            "curr_tweet_sentiment: [0.6610266  0.322822   0.01615136]\n",
            "prev_reddit_sentiment:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at SamLowe/roberta-base-go_emotions and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1) admiration 0.5544\n",
            "2) love 0.2888\n",
            "3) approval 0.0619\n",
            "4) caring 0.023\n",
            "5) neutral 0.0127\n",
            "6) optimism 0.0125\n",
            "7) disapproval 0.0091\n",
            "8) annoyance 0.0063\n",
            "9) anger 0.0055\n",
            "10) disappointment 0.004\n",
            "11) gratitude 0.0036\n",
            "12) desire 0.0036\n",
            "13) realization 0.0026\n",
            "14) pride 0.0022\n",
            "15) sadness 0.0021\n",
            "16) joy 0.0013\n",
            "17) excitement 0.0011\n",
            "18) disgust 0.001\n",
            "19) fear 0.0007\n",
            "20) relief 0.0006\n",
            "21) confusion 0.0006\n",
            "22) grief 0.0005\n",
            "23) remorse 0.0005\n",
            "24) curiosity 0.0004\n",
            "25) surprise 0.0003\n",
            "26) nervousness 0.0003\n",
            "27) embarrassment 0.0002\n",
            "28) amusement 0.0001\n",
            "prev_tweet_sector:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at cardiffnlp/tweet-topic-latest-multi and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1) science_&_technology 0.9853\n",
            "2) news_&_social_concern 0.0066\n",
            "3) business_&_entrepreneurs 0.0027\n",
            "4) other_hobbies 0.0008\n",
            "5) diaries_&_daily_life 0.0007\n",
            "6) learning_&_educational 0.0006\n",
            "7) film_tv_&_video 0.0005\n",
            "8) fitness_&_health 0.0004\n",
            "9) celebrity_&_pop_culture 0.0004\n",
            "10) gaming 0.0003\n",
            "11) travel_&_adventure 0.0003\n",
            "12) sports 0.0002\n",
            "13) relationships 0.0002\n",
            "14) youth_&_student_life 0.0002\n",
            "15) music 0.0002\n",
            "16) food_&_dining 0.0002\n",
            "17) arts_&_culture 0.0002\n",
            "18) family 0.0002\n",
            "19) fashion_&_style 0.0001\n",
            "curr_tweet_sector:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at cardiffnlp/tweet-topic-latest-multi and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1) news_&_social_concern 0.9976\n",
            "2) science_&_technology 0.001\n",
            "3) celebrity_&_pop_culture 0.0003\n",
            "4) film_tv_&_video 0.0003\n",
            "5) other_hobbies 0.0002\n",
            "6) gaming 0.0001\n",
            "7) diaries_&_daily_life 0.0001\n",
            "8) business_&_entrepreneurs 0.0001\n",
            "9) learning_&_educational 0.0001\n",
            "10) sports 0.0001\n",
            "11) arts_&_culture 0.0\n",
            "12) music 0.0\n",
            "13) youth_&_student_life 0.0\n",
            "14) fitness_&_health 0.0\n",
            "15) travel_&_adventure 0.0\n",
            "16) family 0.0\n",
            "17) relationships 0.0\n",
            "18) food_&_dining 0.0\n",
            "19) fashion_&_style 0.0\n"
          ]
        }
      ],
      "source": [
        "test()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch\n",
        "import numpy as np "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### dataset loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ],
      "source": [
        "\n",
        "class dataloaderDUNEs(Dataset):\n",
        "    def __init__(self, data, preprocessing_model):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            data (List[Dict]): Each dictionary contains raw text for 'prev_tweet', 'curr_tweet',\n",
        "                               'prev_reddit', and engagement metrics ('likes', 'retweets', 'comments').\n",
        "            preprocessing_model (PreDUNES): The model instance for preprocessing text data.\n",
        "        \"\"\"\n",
        "        self.data = data\n",
        "        self.preprocessing_model = preprocessing_model\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        item = self.data[idx]\n",
        "        # Process the raw texts through the PreDUNES model\n",
        "        # Note: Ensure PreDUNES outputs PyTorch tensors; if it outputs numpy arrays, convert them to tensors\n",
        "        prev_tweet_embedding, curr_tweet_embedding, prev_tweet_sentiment, curr_tweet_sentiment, prev_reddit_sentiment, prev_tweet_sector, curr_tweet_sector = self.preprocessing_model(\n",
        "            item['prev_tweet'], item['curr_tweet'], item['prev_reddit']\n",
        "        )\n",
        "\n",
        "        # Convert numpy arrays to tensors if they are not already\n",
        "        def to_tensor(array):\n",
        "            if isinstance(array, np.ndarray):\n",
        "                return torch.tensor(array).float()\n",
        "            return array\n",
        "        \n",
        "        prev_tweet_embedding = to_tensor(prev_tweet_embedding)\n",
        "        curr_tweet_embedding = to_tensor(curr_tweet_embedding)\n",
        "        prev_tweet_sentiment = to_tensor(prev_tweet_sentiment)\n",
        "        curr_tweet_sentiment = to_tensor(curr_tweet_sentiment)\n",
        "        prev_reddit_sentiment = to_tensor(prev_reddit_sentiment)\n",
        "        prev_tweet_sector = to_tensor(prev_tweet_sector)\n",
        "        curr_tweet_sector = to_tensor(curr_tweet_sector)\n",
        "        \n",
        "        # Engagement metrics\n",
        "        likes = torch.tensor(item['likes'], dtype=torch.float)\n",
        "        retweets = torch.tensor(item['retweets'], dtype=torch.float)\n",
        "        comments = torch.tensor(item['comments'], dtype=torch.float)\n",
        "        \n",
        "        return {\n",
        "            'prev_tweet_embedding': prev_tweet_embedding,\n",
        "            'curr_tweet_embedding': curr_tweet_embedding,\n",
        "            'prev_tweet_sentiment': prev_tweet_sentiment,\n",
        "            'curr_tweet_sentiment': curr_tweet_sentiment,\n",
        "            'prev_reddit_sentiment': prev_reddit_sentiment,\n",
        "            'prev_tweet_sector': prev_tweet_sector,\n",
        "            'curr_tweet_sector': curr_tweet_sector,\n",
        "            'likes': likes,\n",
        "            'retweets': retweets,\n",
        "            'comments': comments\n",
        "        }\n",
        "\n",
        "# Initialize PreDUNES model here\n",
        "preprocessing_model = create_preprocessing_model(\n",
        "    \"mixedbread-ai/mxbai-embed-large-v1\",\n",
        "    \"cardiffnlp/twitter-roberta-base-sentiment-latest\",\n",
        "    \"SamLowe/roberta-base-go_emotions\",\n",
        "    \"cardiffnlp/tweet-topic-latest-multi\"\n",
        ")\n",
        "\n",
        "# Example data list\n",
        "data = [\n",
        "    {\n",
        "        'prev_tweet': \"@WholeMarsBlog Headline is misleading. Starlink can obviously offer far more robust positioning than GPS, as it will have ~1000X more satellites over time. Not all will have line of sight to users, but still >10X GPS & far stronger signal. Just not today’s problem.\",\n",
        "        'curr_tweet': \"@spideycyp_155 @BillyM2k If Russia faced calamitous defeat in conventional warfare for something as strategically critical as Crimea, the probability of using nuclear weapons is high\",\n",
        "        'prev_reddit': \"We know who controls the media. The same corporations who have wreaked havoc on the globe for decades, if not centuries, the big banks who financed them, and the governments who turned a blind eye to the destruction. The same entities who have brought us to the precipice of destruction - quite possibly condemning us, and our progeny to an unlivable climate They have tried to stop you at every turn, and yet you persist for the good of humanity. We love you, Elon! Keep up the good work! As you have said, we must never let the light of human consciousness fade - never!\",\n",
        "        'likes': 100,  \n",
        "        'retweets': 50, \n",
        "        'comments': 25  \n",
        "    }\n",
        "]\n",
        "\n",
        "# Create dataset and dataloader\n",
        "dataset = dataloaderDUNEs(data, preprocessing_model)\n",
        "dataloader = DataLoader(dataset, batch_size=32, shuffle=True, num_workers=0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [],
      "source": [
        "class EngagementPredictionModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(EngagementPredictionModel, self).__init__()\n",
        "        self.fc1 = nn.Linear(2120, 512)  \n",
        "        self.fc2 = nn.Linear(512, 256)\n",
        "        self.fc3 = nn.Linear(256, 3) \n",
        "\n",
        "    def forward(self, concatenated_features):\n",
        "        x = F.relu(self.fc1(concatenated_features))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "model = EngagementPredictionModel()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/25], Loss: 3185.853271484375\n",
            "Epoch [2/25], Loss: 2744.172607421875\n",
            "Epoch [3/25], Loss: 2252.432861328125\n",
            "Epoch [4/25], Loss: 1730.6173095703125\n",
            "Epoch [5/25], Loss: 1209.2371826171875\n",
            "Epoch [6/25], Loss: 730.9028930664062\n",
            "Epoch [7/25], Loss: 351.5458679199219\n",
            "Epoch [8/25], Loss: 133.73817443847656\n",
            "Epoch [9/25], Loss: 116.4188003540039\n",
            "Epoch [10/25], Loss: 256.1673889160156\n",
            "Epoch [11/25], Loss: 427.4223937988281\n",
            "Epoch [12/25], Loss: 510.9857177734375\n",
            "Epoch [13/25], Loss: 472.5160217285156\n",
            "Epoch [14/25], Loss: 351.9346618652344\n",
            "Epoch [15/25], Loss: 211.14027404785156\n",
            "Epoch [16/25], Loss: 98.5987777709961\n",
            "Epoch [17/25], Loss: 37.24009323120117\n",
            "Epoch [18/25], Loss: 26.705459594726562\n",
            "Epoch [19/25], Loss: 52.2209358215332\n",
            "Epoch [20/25], Loss: 94.04297637939453\n",
            "Epoch [21/25], Loss: 135.4239501953125\n",
            "Epoch [22/25], Loss: 164.58758544921875\n",
            "Epoch [23/25], Loss: 175.559326171875\n",
            "Epoch [24/25], Loss: 166.2635040283203\n",
            "Epoch [25/25], Loss: 140.596435546875\n"
          ]
        }
      ],
      "source": [
        "# Loss and optimizer\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 25\n",
        "for epoch in range(num_epochs):\n",
        "    for batch in dataloader:\n",
        "        def process_tensor(tensor):\n",
        "            tensor = tensor.squeeze()\n",
        "            # Ensure tensor is at least 2D (batch_size, features)\n",
        "            if tensor.dim() == 1:\n",
        "                tensor = tensor.unsqueeze(0)\n",
        "            return tensor\n",
        "        \n",
        "        # Apply this processing to all tensors\n",
        "        prev_tweet_embedding = process_tensor(batch['prev_tweet_embedding'])\n",
        "        curr_tweet_embedding = process_tensor(batch['curr_tweet_embedding'])\n",
        "        prev_tweet_sentiment = process_tensor(batch['prev_tweet_sentiment'])\n",
        "        curr_tweet_sentiment = process_tensor(batch['curr_tweet_sentiment'])\n",
        "        prev_reddit_sentiment = process_tensor(batch['prev_reddit_sentiment'])\n",
        "        prev_tweet_sector = process_tensor(batch['prev_tweet_sector'])\n",
        "        curr_tweet_sector = process_tensor(batch['curr_tweet_sector'])\n",
        "        \n",
        "        # Now, concatenate along dim=1 as all tensors are guaranteed to be at least 2D\n",
        "        concatenated_features = torch.cat((\n",
        "            prev_tweet_embedding, curr_tweet_embedding,\n",
        "            prev_tweet_sentiment, curr_tweet_sentiment,\n",
        "            prev_reddit_sentiment,\n",
        "            prev_tweet_sector, curr_tweet_sector\n",
        "        ), dim=1)\n",
        "\n",
        "        # Prepare targets\n",
        "        targets = torch.stack((batch['likes'], batch['retweets'], batch['comments']), dim=1)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(concatenated_features)\n",
        "\n",
        "        # Compute loss\n",
        "        loss = criterion(outputs, targets)\n",
        "\n",
        "        # Backward and optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item()}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "def compute_accuracy(targets, predictions, threshold=10):\n",
        "    \"\"\"\n",
        "    Computes a simple accuracy metric based on whether the predicted values\n",
        "    are within a certain range (threshold) of the actual values.\n",
        "    \"\"\"\n",
        "    correct = (torch.abs(targets - predictions) <= threshold).all(dim=1)\n",
        "    accuracy = torch.mean(correct.float()) * 100\n",
        "    return accuracy\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 25\n",
        "for epoch in range(num_epochs):\n",
        "    epoch_losses = []\n",
        "    epoch_accuracies = []\n",
        "    for batch in dataloader:\n",
        "        # Concatenate features, prepare targets, forward pass, compute loss, backward pass, and optimize\n",
        "        # Similar to the provided snippet above\n",
        "        \n",
        "        # After loss.backward() and optimizer.step(), compute accuracy\n",
        "        with torch.no_grad():  # Ensure no computation is recorded for gradient purposes\n",
        "            # Assuming your model outputs predictions directly comparable to 'targets'\n",
        "            accuracy = compute_accuracy(targets, outputs)\n",
        "            epoch_accuracies.append(accuracy.item())\n",
        "        \n",
        "        epoch_losses.append(loss.item())\n",
        "    \n",
        "    # Compute the average loss and accuracy for the epoch\n",
        "    epoch_loss = sum(epoch_losses) / len(epoch_losses)\n",
        "    epoch_accuracy = sum(epoch_accuracies) / len(epoch_accuracies)\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.2f}%')\n",
        "\n",
        "    # Optionally, you can print predicted engagement metrics for the last batch\n",
        "    if epoch == num_epochs - 1:  # Or choose any other epoch or condition for displaying\n",
        "        print(\"Actual Engagement Metrics (Last Batch):\", targets[-1].tolist())  # Last item in the batch\n",
        "        print(\"Predicted Engagement Metrics (Last Batch):\", outputs[-1].tolist())\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
